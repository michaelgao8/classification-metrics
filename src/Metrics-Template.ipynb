{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Default Arguments \n",
    "save_figures = True\n",
    "output_dir = \"../test/\"\n",
    "input_file = \"../data/plot_data.json\"\n",
    "model_results = \"./data/test.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Setup -- Make sure to execute this block to enable thresholding tool\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from ipywidgets import interact\n",
    "plt.style.use('ggplot')\n",
    "with open(input_file, 'r') as f:\n",
    "    plot_dict = json.load(f)\n",
    "results = pd.read_csv(model_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Model Metrics\n",
    "\n",
    "The following graphs have been automatically generated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## AUROC (Area Under Receiver Operating Characteristic) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 9)); plt.plot(plot_dict['ROC']['fpr'], plot_dict['ROC']['tpr'], linewidth = 2); plt.title('ROC Curve', size = 20); plt.plot([0, 1], [0, 1], 'k--'); plt.annotate('AUROC: {:.2f} ({:.2f}, {:.2f})'.format(plot_dict['ROC']['auc'], plot_dict['ROC']['auc_lower_bound'], plot_dict['ROC']['auc_upper_bound']), [0.6, 0.4], fontsize = 16); plt.ylabel('True Positive Rate (Sensitivity, Recall)', size = 16); plt.xlabel('False Positive Rate', size = 16)\n",
    "if save_figures: plt.savefig(output_dir + 'auroc.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Class Probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 9));sns.distplot(results.iloc[results.iloc[:, 0].values == 0, 1], kde = False, norm_hist = True, label = \"Negative\", bins = 30);sns.distplot(results.iloc[results.iloc[:, 0].values == 1, 1], kde = False, norm_hist = True, label = \"Positive\", bins = 30);plt.legend();plt.title(\"Distributions by class\", size = 20);plt.xlabel(\"Predicted Probability\", size = 16);plt.ylabel(\"Normalized Frequency\", size = 16)\n",
    "if save_figures: plt.savefig(output_dir + 'class_probabilities.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Positive Predictive Value by Decile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 9));sns.boxplot([1 - x for x in plot_dict['pid']['decile_midpoint']], plot_dict['pid']['pid']);plt.xticks(range(10), [0.95, 0.85, 0.75, 0.65, 0.55, 0.45, 0.35, 0.25, 0.15, 0.05]);plt.title('Positive Predictive Value in Decile of Risk', size = 20);plt.xlabel('Risk decile midpoint', size = 16);plt.ylabel('Positive Predictive Value (Precision)', size = 16)\n",
    "if save_figures: plt.savefig(output_dir + 'ppv_decile.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##  Precision @ k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 9));plt.plot(plot_dict['precision_at_k']['cutpoints'], plot_dict['precision_at_k']['precision_at_k'], linewidth = 2);plt.title('Precision @ k', size = 20);plt.ylabel('Precision', size = 16);plt.xlabel('Top k risk (percent)', size = 16)\n",
    "if save_figures: plt.savefig(output_dir + 'precision_at_k.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Precision-Recall "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 9));plt.plot(plot_dict['avg_precision']['recall'], plot_dict['avg_precision']['precision'], linewidth = 2);plt.title('Precision-Recall Curve', size = 20);plt.annotate('Average Precision: {:.2f} ({:.2f}, {:.2f})'.format(plot_dict['avg_precision']['avg_precision'], plot_dict['avg_precision']['avg_precision_lower_bound'], plot_dict['avg_precision']['avg_precision_upper_bound']), [0.4, 0.8], fontsize = 16);plt.ylabel('Positive Predictive Value (Precision)', size = 16);plt.xlabel('Recall', size = 16)\n",
    "if save_figures: plt.savefig(output_dir + 'precision_recall.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 9));plt.plot(plot_dict['calibration']['prob_pred'], plot_dict['calibration']['prob_true'], linewidth = 2);plt.title('Calibration Curve', size = 20);plt.plot([0, 1], [0, 1], 'k--');plt.ylabel('Empirical Probability', size = 16);plt.xlabel('Predicted Probability', size = 16)\n",
    "if save_figures: plt.savefig(output_dir + 'calibration.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Thresholding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def _generate_confusion_matrix(true, pred, threshold):\n",
    "    true_series = pd.Series(true, name = 'True Label')\n",
    "    pred_series = pd.Series([1 if x > threshold else 0 for x in pred], name = 'Predicted Label')\n",
    "    return pd.crosstab(true_series, pred_series)\n",
    "\n",
    "def _get_metrics(confusion_matrix):\n",
    "    tp = confusion_matrix.loc[1,1]\n",
    "    fp = confusion_matrix.loc[0, 1]\n",
    "    tn = confusion_matrix.loc[0, 0]\n",
    "    fn = confusion_matrix.loc[1, 0]\n",
    "    prevalence = (tp + fn) / (tp + fp + tn + fn)\n",
    "    sensitivity = tp / (tp + fn)\n",
    "    specificity  = tn / (tn + fp)\n",
    "    ppv = tp / (tp + fp)\n",
    "    npv = tn / (tn + fn)\n",
    "    \n",
    "    return tp, fp, tn, fn, prevalence, sensitivity, specificity, ppv, npv\n",
    "\n",
    "true = results.iloc[:, 0]\n",
    "pred = results.iloc[:, 1]\n",
    "\n",
    "def print_metrics(threshold = 0.5):\n",
    "    # Confusion_matrix\n",
    "    print(\"Confusion Matrix at Threshold of {:.4f}\".format(threshold))\n",
    "    print(\"\\n\")\n",
    "    conf_mat = _generate_confusion_matrix(true, pred, threshold)\n",
    "    print(conf_mat)\n",
    "    tp, fp, tn, fn, prevalence, sensitivity, specificity, ppv, npv = _get_metrics(conf_mat)\n",
    "    print(\"\\n\")\n",
    "    print(\"=====================\")\n",
    "    print(\"Metrics:\")\n",
    "    print(\"=====================\")\n",
    "    print(\"Prevalence of Outcome: {:.4f}\".format(prevalence))\n",
    "    print(\"Sensitivity / Recall / True Positive Rate: {:.4f}\".format(sensitivity))\n",
    "    print(\"Specificity / True Negative Rate: {:.4f}\".format(specificity))\n",
    "    print(\"Positive predictive Value / Precision: {:.4f}\".format(ppv))\n",
    "    print(\"Negative Predictive Value: {:.4f}\".format(npv))\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "interact(print_metrics, threshold = (0.01, 0.99, 0.01))"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "rise": {
   "autolaunch": true,
   "height": "100%",
   "scroll": true,
   "theme": "serif",
   "width": "100%"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
